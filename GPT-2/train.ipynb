{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1d7d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:21.880425Z",
     "iopub.status.busy": "2024-10-11T21:33:21.880112Z",
     "iopub.status.idle": "2024-10-11T21:33:34.627585Z",
     "shell.execute_reply": "2024-10-11T21:33:34.626640Z"
    },
    "papermill": {
     "duration": 12.755359,
     "end_time": "2024-10-11T21:33:34.629992",
     "exception": false,
     "start_time": "2024-10-11T21:33:21.874633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\r\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e53bcff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:34.640581Z",
     "iopub.status.busy": "2024-10-11T21:33:34.640245Z",
     "iopub.status.idle": "2024-10-11T21:33:37.897712Z",
     "shell.execute_reply": "2024-10-11T21:33:37.896914Z"
    },
    "papermill": {
     "duration": 3.265216,
     "end_time": "2024-10-11T21:33:37.899943",
     "exception": false,
     "start_time": "2024-10-11T21:33:34.634727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c744a78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:37.910539Z",
     "iopub.status.busy": "2024-10-11T21:33:37.910017Z",
     "iopub.status.idle": "2024-10-11T21:33:37.948438Z",
     "shell.execute_reply": "2024-10-11T21:33:37.947745Z"
    },
    "papermill": {
     "duration": 0.045822,
     "end_time": "2024-10-11T21:33:37.950248",
     "exception": false,
     "start_time": "2024-10-11T21:33:37.904426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Projection of inputs so that we can get key, query and value\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Output Linear Layer\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # For scaling during initialization\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimension\n",
    "\n",
    "        qkv = self.c_attn(x)  # Performing forward pass to get concatenated output of query, key and value\n",
    "        q, k, v = qkv.split(self.n_embd, dim=-1)  # Splitting into query, key and value\n",
    "        # Creating a number of heads dimension in between\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        # Apply attention mechanism\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # Forward pass on the output layer\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257  # Vocabulary size (256 byte tokens + 50000 BPE merges + <|endoftext|> token\n",
    "    n_layer: int = 12  # Number of Block Layers\n",
    "    n_head: int = 12  # Number of Heads\n",
    "    n_embd: int = 768  # Embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:  # Softmax is applied explicitly in F.cross_entropy loss function\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
    "            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
    "            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type, master_process=True):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3f1c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:37.959760Z",
     "iopub.status.busy": "2024-10-11T21:33:37.959496Z",
     "iopub.status.idle": "2024-10-11T21:33:37.966268Z",
     "shell.execute_reply": "2024-10-11T21:33:37.965491Z"
    },
    "papermill": {
     "duration": 0.01398,
     "end_time": "2024-10-11T21:33:37.968461",
     "exception": false,
     "start_time": "2024-10-11T21:33:37.954481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        # Loading Tokens\n",
    "        tokens = np.load('/kaggle/input/fineweb-100m/tokenized_100M.npy')\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position:self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        self.current_position += B * T\n",
    "\n",
    "        if self.current_position + B * T >= len(self.tokens):\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14932f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:37.977919Z",
     "iopub.status.busy": "2024-10-11T21:33:37.977639Z",
     "iopub.status.idle": "2024-10-11T21:33:44.964156Z",
     "shell.execute_reply": "2024-10-11T21:33:44.963211Z"
    },
    "papermill": {
     "duration": 6.993832,
     "end_time": "2024-10-11T21:33:44.966509",
     "exception": false,
     "start_time": "2024-10-11T21:33:37.972677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size : 524288\n",
      "=> calculated gradient accumulated steps : 64\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524288\n",
    "B = 8 # mini batch size\n",
    "T = 1024 # max sequence length\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size : {total_batch_size}\")\n",
    "print(f\"=> calculated gradient accumulated steps : {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoader(B, T)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "# model = torch.compile(model) Needed GPU A100 or latest (Kaggle GPUs do not work)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341c4180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:44.977173Z",
     "iopub.status.busy": "2024-10-11T21:33:44.976516Z",
     "iopub.status.idle": "2024-10-11T21:33:44.983522Z",
     "shell.execute_reply": "2024-10-11T21:33:44.982711Z"
    },
    "papermill": {
     "duration": 0.014302,
     "end_time": "2024-10-11T21:33:44.985483",
     "exception": false,
     "start_time": "2024-10-11T21:33:44.971181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory to save logs\n",
    "log_dir = \"log\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "\n",
    "# Directory to save checkpoints\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "# Function to save a checkpoint\n",
    "def save_checkpoint(step, model, optimizer, loss_acc, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"gpt2_checkpoint_step_{step}.pt\")\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss_acc,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08f1624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:44.996726Z",
     "iopub.status.busy": "2024-10-11T21:33:44.996073Z",
     "iopub.status.idle": "2024-10-11T21:33:45.001923Z",
     "shell.execute_reply": "2024-10-11T21:33:45.001153Z"
    },
    "papermill": {
     "duration": 0.012869,
     "end_time": "2024-10-11T21:33:45.003790",
     "exception": false,
     "start_time": "2024-10-11T21:33:44.990921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_steps = 200\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 50\n",
    "\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    elif step > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baecc7e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T21:33:45.013570Z",
     "iopub.status.busy": "2024-10-11T21:33:45.013294Z",
     "iopub.status.idle": "2024-10-12T01:24:19.855454Z",
     "shell.execute_reply": "2024-10-12T01:24:19.854404Z"
    },
    "papermill": {
     "duration": 13834.849521,
     "end_time": "2024-10-12T01:24:19.857670",
     "exception": false,
     "start_time": "2024-10-11T21:33:45.008149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.9881, Norm : 15.223847389221191, dt : 69367.71ms, tokens/sec : 7558.10s\n",
      "Step 1, Loss: 10.4542, Norm : 9.434684753417969, dt : 67871.64ms, tokens/sec : 7724.70s\n",
      "Step 2, Loss: 10.0339, Norm : 5.908762454986572, dt : 67925.67ms, tokens/sec : 7718.55s\n",
      "Step 3, Loss: 9.7504, Norm : 3.7307612895965576, dt : 67913.06ms, tokens/sec : 7719.99s\n",
      "Step 4, Loss: 9.6447, Norm : 2.669076919555664, dt : 67970.16ms, tokens/sec : 7713.50s\n",
      "Step 5, Loss: 9.5652, Norm : 2.356815814971924, dt : 67994.51ms, tokens/sec : 7710.74s\n",
      "Step 6, Loss: 9.4975, Norm : 2.2452168464660645, dt : 67957.24ms, tokens/sec : 7714.97s\n",
      "Step 7, Loss: 9.4421, Norm : 2.147087335586548, dt : 67952.32ms, tokens/sec : 7715.53s\n",
      "Step 8, Loss: 9.3399, Norm : 2.106595993041992, dt : 67958.68ms, tokens/sec : 7714.81s\n",
      "Step 9, Loss: 9.2240, Norm : 2.0408270359039307, dt : 67894.13ms, tokens/sec : 7722.14s\n",
      "Step 10, Loss: 9.0883, Norm : 2.02877140045166, dt : 67965.41ms, tokens/sec : 7714.04s\n",
      "Step 11, Loss: 9.0276, Norm : 1.858809471130371, dt : 67913.03ms, tokens/sec : 7719.99s\n",
      "Step 12, Loss: 8.8981, Norm : 2.4880592823028564, dt : 67955.09ms, tokens/sec : 7715.21s\n",
      "Step 13, Loss: 8.9015, Norm : 4.639449596405029, dt : 67994.78ms, tokens/sec : 7710.71s\n",
      "Step 14, Loss: 8.7258, Norm : 1.9424172639846802, dt : 67968.01ms, tokens/sec : 7713.75s\n",
      "Step 15, Loss: 8.6251, Norm : 1.9780032634735107, dt : 67985.28ms, tokens/sec : 7711.79s\n",
      "Step 16, Loss: 8.4537, Norm : 1.6712199449539185, dt : 67996.45ms, tokens/sec : 7710.52s\n",
      "Step 17, Loss: 8.3302, Norm : 2.205404758453369, dt : 67983.89ms, tokens/sec : 7711.94s\n",
      "Step 18, Loss: 8.2223, Norm : 1.4218573570251465, dt : 68022.66ms, tokens/sec : 7707.55s\n",
      "Step 19, Loss: 8.1164, Norm : 1.6549047231674194, dt : 68032.79ms, tokens/sec : 7706.40s\n",
      "Step 20, Loss: 8.0039, Norm : 1.2582998275756836, dt : 68029.41ms, tokens/sec : 7706.78s\n",
      "Step 21, Loss: 7.8724, Norm : 1.3094407320022583, dt : 67947.62ms, tokens/sec : 7716.06s\n",
      "Step 22, Loss: 7.7974, Norm : 1.0167673826217651, dt : 67961.35ms, tokens/sec : 7714.50s\n",
      "Step 23, Loss: 7.6619, Norm : 0.9572250843048096, dt : 67941.03ms, tokens/sec : 7716.81s\n",
      "Step 24, Loss: 7.6270, Norm : 0.7226536273956299, dt : 67952.94ms, tokens/sec : 7715.46s\n",
      "Checkpoint saved at step 24\n",
      "Step 25, Loss: 7.5208, Norm : 0.8767789602279663, dt : 68021.00ms, tokens/sec : 7707.74s\n",
      "Step 26, Loss: 7.4908, Norm : 0.7028678059577942, dt : 67975.83ms, tokens/sec : 7712.86s\n",
      "Step 27, Loss: 7.4413, Norm : 0.5876810550689697, dt : 67974.69ms, tokens/sec : 7712.99s\n",
      "Step 28, Loss: 7.3772, Norm : 0.7698952555656433, dt : 68017.64ms, tokens/sec : 7708.12s\n",
      "Step 29, Loss: 7.3231, Norm : 0.849143385887146, dt : 68055.73ms, tokens/sec : 7703.80s\n",
      "Step 30, Loss: 7.3672, Norm : 0.5654592514038086, dt : 68024.25ms, tokens/sec : 7707.37s\n",
      "Step 31, Loss: 7.3280, Norm : 0.7866339087486267, dt : 67979.38ms, tokens/sec : 7712.46s\n",
      "Step 32, Loss: 7.2365, Norm : 0.6876797080039978, dt : 68002.17ms, tokens/sec : 7709.87s\n",
      "Step 33, Loss: 7.3063, Norm : 0.7107402086257935, dt : 68052.27ms, tokens/sec : 7704.20s\n",
      "Step 34, Loss: 7.2312, Norm : 0.877438485622406, dt : 68052.97ms, tokens/sec : 7704.12s\n",
      "Step 35, Loss: 7.2026, Norm : 1.007067322731018, dt : 68014.30ms, tokens/sec : 7708.50s\n",
      "Step 36, Loss: 7.1445, Norm : 0.7420728206634521, dt : 68078.53ms, tokens/sec : 7701.22s\n",
      "Step 37, Loss: 7.2524, Norm : 1.214513897895813, dt : 68025.04ms, tokens/sec : 7707.28s\n",
      "Step 38, Loss: 7.0994, Norm : 1.2869688272476196, dt : 68049.81ms, tokens/sec : 7704.47s\n",
      "Step 39, Loss: 7.1289, Norm : 1.0096497535705566, dt : 68037.86ms, tokens/sec : 7705.83s\n",
      "Step 40, Loss: 7.1401, Norm : 0.741736888885498, dt : 68037.91ms, tokens/sec : 7705.82s\n",
      "Step 41, Loss: 7.0231, Norm : 0.6705507636070251, dt : 68058.34ms, tokens/sec : 7703.51s\n",
      "Step 42, Loss: 7.0676, Norm : 0.8931551575660706, dt : 68029.51ms, tokens/sec : 7706.77s\n",
      "Step 43, Loss: 7.0392, Norm : 0.9686673879623413, dt : 68067.94ms, tokens/sec : 7702.42s\n",
      "Step 44, Loss: 6.9522, Norm : 1.1995735168457031, dt : 68037.11ms, tokens/sec : 7705.91s\n",
      "Step 45, Loss: 6.8694, Norm : 0.9661712050437927, dt : 68048.77ms, tokens/sec : 7704.59s\n",
      "Step 46, Loss: 6.9238, Norm : 1.238226294517517, dt : 67993.50ms, tokens/sec : 7710.85s\n",
      "Step 47, Loss: 6.9186, Norm : 1.1403971910476685, dt : 68020.49ms, tokens/sec : 7707.80s\n",
      "Step 48, Loss: 6.9821, Norm : 1.1087241172790527, dt : 68096.88ms, tokens/sec : 7699.15s\n",
      "Step 49, Loss: 6.9833, Norm : 0.9229027032852173, dt : 68027.07ms, tokens/sec : 7707.05s\n",
      "Checkpoint saved at step 49\n",
      "Step 50, Loss: 6.9687, Norm : 0.6723604202270508, dt : 68060.59ms, tokens/sec : 7703.25s\n",
      "Step 51, Loss: 6.9005, Norm : 0.5706511735916138, dt : 68052.96ms, tokens/sec : 7704.12s\n",
      "Step 52, Loss: 6.9289, Norm : 0.49044501781463623, dt : 68018.09ms, tokens/sec : 7708.07s\n",
      "Step 53, Loss: 6.9610, Norm : 0.8544248938560486, dt : 68012.84ms, tokens/sec : 7708.66s\n",
      "Step 54, Loss: 6.8542, Norm : 0.7565051913261414, dt : 68026.05ms, tokens/sec : 7707.16s\n",
      "Step 55, Loss: 6.8646, Norm : 0.9532870650291443, dt : 68065.66ms, tokens/sec : 7702.68s\n",
      "Step 56, Loss: 6.8904, Norm : 1.6522599458694458, dt : 68008.77ms, tokens/sec : 7709.12s\n",
      "Step 57, Loss: 6.8342, Norm : 0.9505842924118042, dt : 68009.40ms, tokens/sec : 7709.05s\n",
      "Step 58, Loss: 6.8121, Norm : 1.2516953945159912, dt : 68036.57ms, tokens/sec : 7705.97s\n",
      "Step 59, Loss: 6.8168, Norm : 0.6607802510261536, dt : 68077.89ms, tokens/sec : 7701.30s\n",
      "Step 60, Loss: 6.7744, Norm : 0.9474769830703735, dt : 68024.95ms, tokens/sec : 7707.29s\n",
      "Step 61, Loss: 6.7741, Norm : 1.150173306465149, dt : 68031.53ms, tokens/sec : 7706.54s\n",
      "Step 62, Loss: 6.8375, Norm : 0.836879312992096, dt : 67995.23ms, tokens/sec : 7710.66s\n",
      "Step 63, Loss: 6.6580, Norm : 1.0030995607376099, dt : 68027.99ms, tokens/sec : 7706.94s\n",
      "Step 64, Loss: 6.6778, Norm : 1.0778647661209106, dt : 67967.79ms, tokens/sec : 7713.77s\n",
      "Step 65, Loss: 6.6544, Norm : 0.7254371047019958, dt : 68021.12ms, tokens/sec : 7707.72s\n",
      "Step 66, Loss: 6.6457, Norm : 0.7030970454216003, dt : 68025.28ms, tokens/sec : 7707.25s\n",
      "Step 67, Loss: 6.6455, Norm : 0.5960951447486877, dt : 68009.81ms, tokens/sec : 7709.01s\n",
      "Step 68, Loss: 6.6490, Norm : 0.7476070523262024, dt : 68059.34ms, tokens/sec : 7703.39s\n",
      "Step 69, Loss: 6.6046, Norm : 0.7810012698173523, dt : 68071.45ms, tokens/sec : 7702.03s\n",
      "Step 70, Loss: 6.6844, Norm : 0.7293646931648254, dt : 68038.82ms, tokens/sec : 7705.72s\n",
      "Step 71, Loss: 6.6345, Norm : 1.2321828603744507, dt : 68010.37ms, tokens/sec : 7708.94s\n",
      "Step 72, Loss: 6.6385, Norm : 1.095166802406311, dt : 68053.70ms, tokens/sec : 7704.03s\n",
      "Step 73, Loss: 6.5335, Norm : 0.5554254055023193, dt : 68095.80ms, tokens/sec : 7699.27s\n",
      "Step 74, Loss: 6.4672, Norm : 0.5799440741539001, dt : 68031.44ms, tokens/sec : 7706.55s\n",
      "Checkpoint saved at step 74\n",
      "Step 75, Loss: 6.4954, Norm : 0.5600074529647827, dt : 68023.07ms, tokens/sec : 7707.50s\n",
      "Step 76, Loss: 6.5371, Norm : 0.45467609167099, dt : 68044.12ms, tokens/sec : 7705.12s\n",
      "Step 77, Loss: 6.4849, Norm : 0.6205635666847229, dt : 68010.85ms, tokens/sec : 7708.89s\n",
      "Step 78, Loss: 6.4789, Norm : 0.6706629991531372, dt : 68058.11ms, tokens/sec : 7703.53s\n",
      "Step 79, Loss: 6.4624, Norm : 0.4953412413597107, dt : 67987.86ms, tokens/sec : 7711.49s\n",
      "Step 80, Loss: 6.5115, Norm : 0.4045901596546173, dt : 68033.43ms, tokens/sec : 7706.33s\n",
      "Step 81, Loss: 6.4664, Norm : 0.3440428078174591, dt : 68018.77ms, tokens/sec : 7707.99s\n",
      "Step 82, Loss: 6.4763, Norm : 0.43572998046875, dt : 68029.50ms, tokens/sec : 7706.77s\n",
      "Step 83, Loss: 6.4504, Norm : 0.40500307083129883, dt : 68017.74ms, tokens/sec : 7708.11s\n",
      "Step 84, Loss: 6.3933, Norm : 0.5263272523880005, dt : 68027.90ms, tokens/sec : 7706.96s\n",
      "Step 85, Loss: 6.3999, Norm : 0.6069387197494507, dt : 67978.94ms, tokens/sec : 7712.51s\n",
      "Step 86, Loss: 6.3450, Norm : 0.589410662651062, dt : 68047.19ms, tokens/sec : 7704.77s\n",
      "Step 87, Loss: 6.4212, Norm : 0.5078353881835938, dt : 68027.44ms, tokens/sec : 7707.01s\n",
      "Step 88, Loss: 6.3519, Norm : 0.6576820611953735, dt : 68014.40ms, tokens/sec : 7708.49s\n",
      "Step 89, Loss: 6.4095, Norm : 0.8800381422042847, dt : 67971.43ms, tokens/sec : 7713.36s\n",
      "Step 90, Loss: 6.3563, Norm : 0.6801681518554688, dt : 67987.60ms, tokens/sec : 7711.52s\n",
      "Step 91, Loss: 6.3648, Norm : 0.653971254825592, dt : 68028.65ms, tokens/sec : 7706.87s\n",
      "Step 92, Loss: 6.3508, Norm : 0.4261797070503235, dt : 67999.89ms, tokens/sec : 7710.13s\n",
      "Step 93, Loss: 6.3349, Norm : 0.4595225751399994, dt : 67893.86ms, tokens/sec : 7722.17s\n",
      "Step 94, Loss: 6.2766, Norm : 0.44898098707199097, dt : 67972.36ms, tokens/sec : 7713.25s\n",
      "Step 95, Loss: 6.3507, Norm : 0.40692105889320374, dt : 68009.32ms, tokens/sec : 7709.06s\n",
      "Step 96, Loss: 6.4482, Norm : 0.45385047793388367, dt : 68031.53ms, tokens/sec : 7706.54s\n",
      "Step 97, Loss: 6.6580, Norm : 0.5087037086486816, dt : 68029.23ms, tokens/sec : 7706.81s\n",
      "Step 98, Loss: 6.4411, Norm : 1.14322829246521, dt : 67992.94ms, tokens/sec : 7710.92s\n",
      "Step 99, Loss: 6.4886, Norm : 1.1422767639160156, dt : 68027.88ms, tokens/sec : 7706.96s\n",
      "Checkpoint saved at step 99\n",
      "Step 100, Loss: 6.4549, Norm : 0.8813475370407104, dt : 67993.56ms, tokens/sec : 7710.85s\n",
      "Step 101, Loss: 6.4557, Norm : 0.6114404201507568, dt : 68035.45ms, tokens/sec : 7706.10s\n",
      "Step 102, Loss: 6.4327, Norm : 0.6125584840774536, dt : 67999.50ms, tokens/sec : 7710.17s\n",
      "Step 103, Loss: 6.4088, Norm : 0.43003249168395996, dt : 68045.67ms, tokens/sec : 7704.94s\n",
      "Step 104, Loss: 6.3837, Norm : 0.6222777962684631, dt : 68009.39ms, tokens/sec : 7709.05s\n",
      "Step 105, Loss: 6.3758, Norm : 0.4848093092441559, dt : 68024.99ms, tokens/sec : 7707.28s\n",
      "Step 106, Loss: 6.3426, Norm : 0.5840847492218018, dt : 68033.96ms, tokens/sec : 7706.27s\n",
      "Step 107, Loss: 6.3341, Norm : 0.527938187122345, dt : 68029.91ms, tokens/sec : 7706.73s\n",
      "Step 108, Loss: 6.4077, Norm : 0.4531411826610565, dt : 67949.80ms, tokens/sec : 7715.81s\n",
      "Step 109, Loss: 6.3503, Norm : 0.6475135087966919, dt : 68016.21ms, tokens/sec : 7708.28s\n",
      "Step 110, Loss: 6.3367, Norm : 0.5677522420883179, dt : 67975.10ms, tokens/sec : 7712.94s\n",
      "Step 111, Loss: 6.3122, Norm : 0.4490486979484558, dt : 68046.38ms, tokens/sec : 7704.86s\n",
      "Step 112, Loss: 6.3230, Norm : 0.48177143931388855, dt : 68017.26ms, tokens/sec : 7708.16s\n",
      "Step 113, Loss: 6.2988, Norm : 0.48544061183929443, dt : 68031.66ms, tokens/sec : 7706.53s\n",
      "Step 114, Loss: 6.1975, Norm : 0.3217349052429199, dt : 67983.34ms, tokens/sec : 7712.01s\n",
      "Step 115, Loss: 6.2486, Norm : 0.4195191264152527, dt : 68020.64ms, tokens/sec : 7707.78s\n",
      "Step 116, Loss: 6.3344, Norm : 0.37176814675331116, dt : 67997.65ms, tokens/sec : 7710.38s\n",
      "Step 117, Loss: 6.3430, Norm : 0.4068859815597534, dt : 68075.35ms, tokens/sec : 7701.58s\n",
      "Step 118, Loss: 6.3393, Norm : 0.39399057626724243, dt : 68045.23ms, tokens/sec : 7704.99s\n",
      "Step 119, Loss: 6.2394, Norm : 0.3307693302631378, dt : 67994.41ms, tokens/sec : 7710.75s\n",
      "Step 120, Loss: 6.1918, Norm : 0.3788398802280426, dt : 68054.06ms, tokens/sec : 7703.99s\n",
      "Step 121, Loss: 6.1980, Norm : 0.315558522939682, dt : 68069.35ms, tokens/sec : 7702.26s\n",
      "Step 122, Loss: 6.2604, Norm : 0.32389122247695923, dt : 68050.18ms, tokens/sec : 7704.43s\n",
      "Step 123, Loss: 6.1991, Norm : 0.4298734664916992, dt : 68059.81ms, tokens/sec : 7703.34s\n",
      "Step 124, Loss: 6.1784, Norm : 0.5232329368591309, dt : 68046.92ms, tokens/sec : 7704.80s\n",
      "Checkpoint saved at step 124\n",
      "Step 125, Loss: 6.2118, Norm : 0.3126285672187805, dt : 67970.51ms, tokens/sec : 7713.46s\n",
      "Step 126, Loss: 6.2294, Norm : 0.3357444405555725, dt : 68023.47ms, tokens/sec : 7707.46s\n",
      "Step 127, Loss: 6.1776, Norm : 0.3505972921848297, dt : 68049.03ms, tokens/sec : 7704.56s\n",
      "Step 128, Loss: 6.2047, Norm : 0.37489601969718933, dt : 68005.69ms, tokens/sec : 7709.47s\n",
      "Step 129, Loss: 6.2463, Norm : 0.32180359959602356, dt : 68022.88ms, tokens/sec : 7707.52s\n",
      "Step 130, Loss: 6.1813, Norm : 0.36449185013771057, dt : 68077.14ms, tokens/sec : 7701.38s\n",
      "Step 131, Loss: 6.1698, Norm : 0.34168899059295654, dt : 67966.18ms, tokens/sec : 7713.95s\n",
      "Step 132, Loss: 6.0957, Norm : 0.36954954266548157, dt : 68033.08ms, tokens/sec : 7706.37s\n",
      "Step 133, Loss: 6.0508, Norm : 0.3800938129425049, dt : 68007.96ms, tokens/sec : 7709.21s\n",
      "Step 134, Loss: 6.1092, Norm : 0.3044739067554474, dt : 67993.79ms, tokens/sec : 7710.82s\n",
      "Step 135, Loss: 6.0803, Norm : 0.26063498854637146, dt : 68009.94ms, tokens/sec : 7708.99s\n",
      "Step 136, Loss: 6.0577, Norm : 0.29397138953208923, dt : 68030.33ms, tokens/sec : 7706.68s\n",
      "Step 137, Loss: 6.0584, Norm : 0.21318437159061432, dt : 67994.03ms, tokens/sec : 7710.79s\n",
      "Step 138, Loss: 6.1289, Norm : 0.3298570215702057, dt : 68027.83ms, tokens/sec : 7706.96s\n",
      "Step 139, Loss: 6.1604, Norm : 0.25950244069099426, dt : 68001.16ms, tokens/sec : 7709.99s\n",
      "Step 140, Loss: 6.0647, Norm : 0.33066651225090027, dt : 68001.52ms, tokens/sec : 7709.95s\n",
      "Step 141, Loss: 6.0853, Norm : 0.3097389042377472, dt : 67979.85ms, tokens/sec : 7712.40s\n",
      "Step 142, Loss: 6.0534, Norm : 0.3143517076969147, dt : 68010.73ms, tokens/sec : 7708.90s\n",
      "Step 143, Loss: 6.0853, Norm : 0.35870757699012756, dt : 68019.49ms, tokens/sec : 7707.91s\n",
      "Step 144, Loss: 6.2246, Norm : 0.27518463134765625, dt : 68052.85ms, tokens/sec : 7704.13s\n",
      "Step 145, Loss: 6.2410, Norm : 0.34956997632980347, dt : 68034.74ms, tokens/sec : 7706.18s\n",
      "Step 146, Loss: 6.1859, Norm : 0.3575044274330139, dt : 67956.49ms, tokens/sec : 7715.05s\n",
      "Step 147, Loss: 6.1898, Norm : 0.2624480128288269, dt : 68001.90ms, tokens/sec : 7709.90s\n",
      "Step 148, Loss: 6.1807, Norm : 0.3288376033306122, dt : 68041.77ms, tokens/sec : 7705.38s\n",
      "Step 149, Loss: 6.1732, Norm : 0.26073843240737915, dt : 68030.85ms, tokens/sec : 7706.62s\n",
      "Checkpoint saved at step 149\n",
      "Step 150, Loss: 6.1867, Norm : 0.2627783417701721, dt : 67955.15ms, tokens/sec : 7715.21s\n",
      "Step 151, Loss: 6.1664, Norm : 0.3712942600250244, dt : 68022.84ms, tokens/sec : 7707.53s\n",
      "Step 152, Loss: 6.1730, Norm : 0.414637953042984, dt : 68011.26ms, tokens/sec : 7708.84s\n",
      "Step 153, Loss: 6.1487, Norm : 0.3742181062698364, dt : 68018.07ms, tokens/sec : 7708.07s\n",
      "Step 154, Loss: 6.1684, Norm : 0.29122087359428406, dt : 68005.07ms, tokens/sec : 7709.54s\n",
      "Step 155, Loss: 6.1293, Norm : 0.32933977246284485, dt : 68033.24ms, tokens/sec : 7706.35s\n",
      "Step 156, Loss: 6.1158, Norm : 0.2929924726486206, dt : 68045.65ms, tokens/sec : 7704.95s\n",
      "Step 157, Loss: 6.0973, Norm : 0.36781805753707886, dt : 68034.06ms, tokens/sec : 7706.26s\n",
      "Step 158, Loss: 6.1064, Norm : 0.2651730477809906, dt : 68020.24ms, tokens/sec : 7707.82s\n",
      "Step 159, Loss: 6.1087, Norm : 0.3402021527290344, dt : 67978.70ms, tokens/sec : 7712.53s\n",
      "Step 160, Loss: 6.1157, Norm : 0.3060844838619232, dt : 68073.17ms, tokens/sec : 7701.83s\n",
      "Step 161, Loss: 6.1589, Norm : 0.2303796112537384, dt : 68009.03ms, tokens/sec : 7709.09s\n",
      "Step 162, Loss: 6.0465, Norm : 0.2767363488674164, dt : 68049.35ms, tokens/sec : 7704.53s\n",
      "Step 163, Loss: 6.0559, Norm : 0.2620184123516083, dt : 68017.28ms, tokens/sec : 7708.16s\n",
      "Step 164, Loss: 6.1475, Norm : 0.24408049881458282, dt : 68071.39ms, tokens/sec : 7702.03s\n",
      "Step 165, Loss: 6.1677, Norm : 0.3284335732460022, dt : 68035.31ms, tokens/sec : 7706.12s\n",
      "Step 166, Loss: 6.0883, Norm : 0.29895806312561035, dt : 68014.37ms, tokens/sec : 7708.49s\n",
      "Step 167, Loss: 6.0341, Norm : 0.3130241930484772, dt : 68042.57ms, tokens/sec : 7705.29s\n",
      "Step 168, Loss: 6.0020, Norm : 0.3019566535949707, dt : 68103.08ms, tokens/sec : 7698.45s\n",
      "Step 169, Loss: 6.0460, Norm : 0.2613104581832886, dt : 67999.24ms, tokens/sec : 7710.20s\n",
      "Step 170, Loss: 6.0343, Norm : 0.2441895604133606, dt : 67961.41ms, tokens/sec : 7714.50s\n",
      "Step 171, Loss: 6.0964, Norm : 0.3095078468322754, dt : 68054.17ms, tokens/sec : 7703.98s\n",
      "Step 172, Loss: 6.0687, Norm : 0.22915713489055634, dt : 68035.85ms, tokens/sec : 7706.05s\n",
      "Step 173, Loss: 5.9738, Norm : 0.3312782943248749, dt : 68013.89ms, tokens/sec : 7708.54s\n",
      "Step 174, Loss: 6.0147, Norm : 0.24583223462104797, dt : 68060.06ms, tokens/sec : 7703.31s\n",
      "Checkpoint saved at step 174\n",
      "Step 175, Loss: 6.0201, Norm : 0.32561877369880676, dt : 68001.42ms, tokens/sec : 7709.96s\n",
      "Step 176, Loss: 5.9767, Norm : 0.2319464087486267, dt : 68021.40ms, tokens/sec : 7707.69s\n",
      "Step 177, Loss: 6.0176, Norm : 0.32544270157814026, dt : 68040.98ms, tokens/sec : 7705.47s\n",
      "Step 178, Loss: 6.0153, Norm : 0.24788855016231537, dt : 68079.96ms, tokens/sec : 7701.06s\n",
      "Step 179, Loss: 5.9501, Norm : 0.2713131904602051, dt : 68014.32ms, tokens/sec : 7708.49s\n",
      "Step 180, Loss: 5.9614, Norm : 0.2573947608470917, dt : 68052.70ms, tokens/sec : 7704.15s\n",
      "Step 181, Loss: 5.9712, Norm : 0.24668236076831818, dt : 67997.67ms, tokens/sec : 7710.38s\n",
      "Step 182, Loss: 5.9698, Norm : 0.2505156099796295, dt : 68050.60ms, tokens/sec : 7704.38s\n",
      "Step 183, Loss: 5.9566, Norm : 0.23306703567504883, dt : 68062.52ms, tokens/sec : 7703.04s\n",
      "Step 184, Loss: 5.9580, Norm : 0.2328651398420334, dt : 68077.71ms, tokens/sec : 7701.32s\n",
      "Step 185, Loss: 5.9506, Norm : 0.32915833592414856, dt : 68055.60ms, tokens/sec : 7703.82s\n",
      "Step 186, Loss: 5.9636, Norm : 0.22096507251262665, dt : 68031.98ms, tokens/sec : 7706.49s\n",
      "Step 187, Loss: 5.9277, Norm : 0.2502077519893646, dt : 68058.54ms, tokens/sec : 7703.49s\n",
      "Step 188, Loss: 5.9492, Norm : 0.22924111783504486, dt : 68055.90ms, tokens/sec : 7703.79s\n",
      "Step 189, Loss: 6.0110, Norm : 0.25068169832229614, dt : 68081.65ms, tokens/sec : 7700.87s\n",
      "Step 190, Loss: 6.0882, Norm : 0.2583684027194977, dt : 68061.73ms, tokens/sec : 7703.12s\n",
      "Step 191, Loss: 6.0600, Norm : 0.23874513804912567, dt : 68068.65ms, tokens/sec : 7702.34s\n",
      "Step 192, Loss: 6.1189, Norm : 0.2415018081665039, dt : 68079.44ms, tokens/sec : 7701.12s\n",
      "Step 193, Loss: 6.0736, Norm : 0.2711043953895569, dt : 68079.08ms, tokens/sec : 7701.16s\n",
      "Step 194, Loss: 6.0658, Norm : 0.21632903814315796, dt : 68025.45ms, tokens/sec : 7707.23s\n",
      "Step 195, Loss: 6.1013, Norm : 0.2948247790336609, dt : 68054.63ms, tokens/sec : 7703.93s\n",
      "Step 196, Loss: 6.0415, Norm : 0.2204894870519638, dt : 68078.45ms, tokens/sec : 7701.23s\n",
      "Step 197, Loss: 6.0575, Norm : 0.19886337220668793, dt : 68047.87ms, tokens/sec : 7704.69s\n",
      "Step 198, Loss: 6.0669, Norm : 0.23821505904197693, dt : 68021.57ms, tokens/sec : 7707.67s\n",
      "Step 199, Loss: 6.0735, Norm : 0.23169803619384766, dt : 68054.11ms, tokens/sec : 7703.99s\n",
      "Checkpoint saved at step 199\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_acc = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "        #     logits, loss = model(x, y) Needed GPU A100 (Kaggle GPUs do not work)\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_acc += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # Get learning rate\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_seconds = (train_loader.B * train_loader.T * grad_accum_steps) / (t1 - t0)\n",
    "    print(\n",
    "        f'Step {step}, Loss: {loss_acc.item():.4f}, Norm : {norm}, dt : {dt:.2f}ms, tokens/sec : {tokens_per_seconds:.2f}s')\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{step} train {loss_acc.item():.6f}\\n\")\n",
    "\n",
    "    if (step + 1) % 25 == 0:\n",
    "        save_checkpoint(step, model, optimizer, loss_acc, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df5712c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T01:24:19.899126Z",
     "iopub.status.busy": "2024-10-12T01:24:19.898777Z",
     "iopub.status.idle": "2024-10-12T01:24:22.054868Z",
     "shell.execute_reply": "2024-10-12T01:24:22.053842Z"
    },
    "papermill": {
     "duration": 2.179215,
     "end_time": "2024-10-12T01:24:22.056990",
     "exception": false,
     "start_time": "2024-10-12T01:24:19.877775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, one of all the long term (a) of these children have been different students in our kids are no children and in mind in this child\n",
      "Hello, we don-up, in my people have the future of this.\n",
      "A�s, or I have to make some of what in\n",
      "Hello, or other time. There is an excellent name, in many important enough-like people who need to be much of the time to all to\n",
      "Hello, the key-t).\n",
      "-\n",
      "- The study? However, or\n",
      "- If we have to use, is. This makes sense\n",
      "Hello, so just\n",
      "If I think about 1, I think I's one’s going on that’m going to a little.\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "# Load the encoder\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# Encode the input text\n",
    "tokens = enc.encode(\"Hello,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Prepare the initial input\n",
    "x = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = x.to(device)  # Assuming device is defined elsewhere\n",
    "\n",
    "# Generate text loop\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        # Get logits and probabilities\n",
    "        logits, _ = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample top k tokens\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # Sample and update input sequence\n",
    "        ix = torch.multinomial(topk_probs, num_samples=1)\n",
    "        xol = torch.gather(topk_indices, dim=-1, index=ix)\n",
    "        x = torch.cat([x, xol], dim=1)\n",
    "\n",
    "# Decode and print generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    text = enc.decode(tokens)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4f301",
   "metadata": {
    "papermill": {
     "duration": 0.019955,
     "end_time": "2024-10-12T01:24:22.097192",
     "exception": false,
     "start_time": "2024-10-12T01:24:22.077237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5847449,
     "sourceId": 9588173,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5859814,
     "sourceId": 9604430,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13864.278335,
   "end_time": "2024-10-12T01:24:23.440559",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-11T21:33:19.162224",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
