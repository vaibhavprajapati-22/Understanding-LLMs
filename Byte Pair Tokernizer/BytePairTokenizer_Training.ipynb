{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:02:39.371169Z",
     "start_time": "2024-09-25T18:02:39.356379Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import wordpunct_tokenize, sent_tokenize\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb8c02a-8bdd-4adf-bdc0-23b5584e6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_vocab(path):\n",
    "    with open(\"data.txt\", 'r') as file: # Reading the text file \n",
    "        data = file.read()\n",
    "    vocab = dict()\n",
    "    data = data.lower()\n",
    "    sentences = sent_tokenize(data) # Creating the list of sentences from the given text corpus\n",
    "    for sentence in sentences:\n",
    "        words = wordpunct_tokenize(sentence) # Creating the list of words present in sentence\n",
    "        for word in words:\n",
    "            vocab[word] = vocab.get(word, 0) + 1 # Increasing the frequency of word by one\n",
    "    bpe_vocab = dict()\n",
    "    for token in vocab:\n",
    "        ntoken = ' '.join(list(token)) + ' </w>' # Splitting the word and joining it with spaces in between and adding a end of word token\n",
    "        bpe_vocab[ntoken] = vocab[token]\n",
    "    return bpe_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a1c00c-3d16-427a-80a7-f2b72cd3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_counts(vocab): # Functions returns the pair count of each unique pair\n",
    "    pairs = dict()\n",
    "    for word in vocab:\n",
    "        contents = word.split(' ')\n",
    "        for u, v in zip(contents[:-1], contents[1:]):\n",
    "            if (u, v) not in pairs:\n",
    "                pairs[(u, v)] = 0\n",
    "            pairs[(u, v)] += vocab[word]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2492732-cae2-411c-a1cb-f066ba0fd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, vocab):\n",
    "    bigram = ' '.join(list(pair)) # Creating a bigram by inserting a space between the pair and joining them\n",
    "    new_vocab = dict()\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') # Regular Expression so that the bigram is between the two white spaces\n",
    "    for word in vocab:\n",
    "        w_out = p.sub(''.join(pair), word) # Forming a new word by replacing the Regex pattern with a single token\n",
    "        new_vocab[w_out] = vocab[word]\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d4528f-7c3e-44d8-9263-0c3eb9d0bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode(vocab): # Create a mapping from token to int and int to token\n",
    "    vocab_to_idx = {}\n",
    "    idx_to_vocab = {}\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True) # Sorting the vocab by count of the words\n",
    "    for i in range(len(sorted_vocab)):\n",
    "        word, count = sorted_vocab[i]\n",
    "        vocab_to_idx[word] = i\n",
    "        idx_to_vocab[i] = word\n",
    "    return vocab_to_idx, idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715335fe-308f-40f7-a5ef-ec9161c83e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token_freq(vocab): # Create a dictionary that contains the count of each token\n",
    "    freq = {}\n",
    "    for word in vocab: # Iterating through the vocab\n",
    "        tokens = word.split() # Creating the list of the tokens in a particular word\n",
    "        for token in tokens: # Iterating through each token and increasing its frequency\n",
    "            if token not in freq:\n",
    "                freq[token] = 0\n",
    "            freq[token] += vocab[word]\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a9c790-3d52-4a71-86fa-1a7ff66c3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(vocab, merges_count): # This fucntion perfomrs merges_count number of merges on vocab and return the mapping from token to int and int to token \n",
    "    for i in range(merges_count):\n",
    "        pairs = get_pair_counts(vocab) # Dictonary that contains the frequency of each pair\n",
    "        max_count_pair = max(pairs, key=pairs.get) # Get the pair that occurs most frequently \n",
    "        vocab = merge_vocab(max_count_pair, vocab) # perform merging of the max occuring pair in the entire vocab\n",
    "    freq = count_token_freq(vocab) # Getting the frequency of the each token\n",
    "    freq[\"<unk>\"] = 1 # Adding the token for unkown charcters\n",
    "    vocab_to_idx, idx_to_vocab = encode_decode(freq) #Getting mapping\n",
    "    return vocab_to_idx, idx_to_vocab, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d1b346-1df7-4786-897e-fa8767d5fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path_text_corpus, path_tokenizer, merges_count):\n",
    "    bpe_vocab = get_bpe_vocab(path_text_corpus)\n",
    "    vocab_to_idx, idx_to_vocab, freqs = train_tokenizer(bpe_vocab, merges_count)\n",
    "    tokenizer_object = BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab)\n",
    "    tokenizer_object.save(path_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab842c1-ffb8-4c8d-9097-4d4fde71d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairTokenizer:\n",
    "\n",
    "    def __init__(self, freqs=None, vocab_to_idx=None, idx_to_vocab=None):\n",
    "        self.freqs = freqs if freqs is not None else {}\n",
    "        self.vocab_to_idx = vocab_to_idx if vocab_to_idx is not None else {}\n",
    "        self.idx_to_vocab = idx_to_vocab if idx_to_vocab is not None else {}\n",
    "        self.unk = \"<unk>\"\n",
    "        self.eow = \"</w>\"\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_to_idx)\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.idx_to_vocab[idx]\n",
    "\n",
    "    def get_id(self, token):\n",
    "        unk_id = self.vocab_to_idx[self.unk]\n",
    "        idx = self.vocab_to_idx[token] if token in self.vocab_to_idx else unk_id\n",
    "        return idx\n",
    "\n",
    "    def get_tokens(self, idxs):\n",
    "        tokens = []\n",
    "        for idx in idxs:\n",
    "            tokens.append(self.idx_to_vocab[idx])\n",
    "        return tokens\n",
    "\n",
    "    def get_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab_to_idx:\n",
    "                ids.append(self.vocab_to_idx[token])\n",
    "            else :\n",
    "                ids.append(self.vocab_to_idx[self.unk])\n",
    "        return ids\n",
    "                \n",
    "    def get_max_pair(self, tokens): # If merge is possible in text then it return the two indices\n",
    "        pairs = {}\n",
    "        for i in range(1, len(tokens)):\n",
    "            pair = ''.join(tokens[i-1:i+1])\n",
    "            if pair in self.freqs:\n",
    "                pairs[(i-1, i)] = self.freqs[pair]\n",
    "        return None if len(pairs)==0 else max(pairs, key=pairs.get)\n",
    "\n",
    "    def merge_max_pair(self, tokens): # Perform Merging\n",
    "        max_pair = self.get_max_pair(tokens)\n",
    "        merged = True if max_pair is not None else False\n",
    "        if merged : \n",
    "            tokens = tokens[:max_pair[0]] + [''.join(tokens[max_pair[0]:max_pair[1]+1])] + tokens[max_pair[1]+1:]\n",
    "        return tokens, merged\n",
    "\n",
    "    def merge_tokens(self, tokens): # Perfomring merging till any merging is possible\n",
    "        tokens, merged = self.merge_max_pair(tokens)\n",
    "        while merged:\n",
    "            tokens, merged = self.merge_max_pair(tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, input):\n",
    "        words = input.split(\" \")\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            word = list(word) + [self.eow]\n",
    "            tokens += self.merge_tokens(word)\n",
    "        enc = self.get_ids(tokens)\n",
    "        return enc\n",
    "\n",
    "    def decode(self, input):\n",
    "        text = []\n",
    "        word = \"\"\n",
    "        tokens = self.get_tokens(input)\n",
    "        for i in range(len(tokens)):\n",
    "            word += tokens[i]\n",
    "            if word.endswith(\"</w>\"):\n",
    "                text.append(word[:-4])\n",
    "                word = \"\"\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def save(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        with open(f\"{path}/freqs.json\", 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(self.freqs, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "        with open(f\"{path}/vocab_to_idx.json\", 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(self.vocab_to_idx, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "        with open(f\"{path}/idx_to_vocab.json\", 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(self.idx_to_vocab, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(f\"{path}/freqs.json\", 'r', encoding='utf-8') as file:\n",
    "            freqs = json.load(file)\n",
    "\n",
    "        with open(f'{path}/vocab_to_idx.json', 'r', encoding='utf-8') as file:\n",
    "            vocab_to_idx = json.load(file)\n",
    "\n",
    "        with open(f'{path}/idx_to_vocab.json', 'r', encoding='utf-8') as file:\n",
    "            idx_to_vocab = json.load(file)\n",
    "\n",
    "        idx_to_vocab = {int(k): v for k, v in idx_to_vocab.items()} # By default json stores keys and values both as strings so we have convert back in to int\n",
    "        \n",
    "        self.freqs = freqs\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.idx_to_vocab = idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d61da428-04f7-4349-b8be-b9dccd3345cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"data.txt\", \"Tokenizer\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aac68ad-348d-44e1-831a-d319ce4654c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE = BytePairTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d3240b-bb79-4cd6-a051-2b534146bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.load(\"Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "603b7527-45c6-4b97-a26a-a1539172f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = BPE.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9879c3c8-27c1-4d39-8eb9-879507d50213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 10, 2, 2, 7, 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6d8768-f09e-4e57-85cf-d989163afb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BPE.decode(enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
